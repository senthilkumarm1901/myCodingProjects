{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb56a3e8",
   "metadata": {},
   "source": [
    "The dockerfile you can use to run this notebook\n",
    "\n",
    "```\n",
    "FROM docker.io/pytorch/pytorch:1.9.0-cuda11.1-cudnn8-devel\n",
    "# https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel_21-04.html#rel_21-04\n",
    "# Cuda 11.1, Py 3.8, PyTorch 1.9, Jupyter notebook and jupyter lab installed\n",
    "\n",
    "# proxy related env settings\n",
    "ENV https_proxy=<your_proxy_url>\n",
    "ENV http_proxy=<your_proxy_url>\n",
    "ENV no_proxy=<your_no_proxy_url>\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "# Set environment variables\n",
    "# the following 2 environment variables are needed to download spacy models\n",
    "ENV LC_ALL=C.UTF-8\n",
    "ENV LANG=C.UTF-8\n",
    "ENV SHELL /bin/bash\n",
    "\n",
    "# solution for fix: https://stackoverflow.com/questions/38002543/apt-get-update-returned-a-non-zero-code-100\n",
    "RUN apt-get install -y apt-transport-https\n",
    "\n",
    "# RUN apt-get update && apt-get install -y --allow-downgrades --allow-change-held-packages --no-install-recommends \\\n",
    "#   build-essential \\\n",
    "#   cmake \\\n",
    "#   g++ \\\n",
    "#   git \\\n",
    "#   curl \\\n",
    "#   vim \\\n",
    "#   wget\n",
    "\n",
    "# to enable scp inside and outside of docker container\n",
    "# RUN apt-get -y update && apt-get install -y openssh-server\n",
    "\n",
    "# before pip installing packages, upgrade, pip, setuptools and wheels\n",
    "RUN pip install -U pip setuptools wheel\n",
    "\n",
    "# for learning torchtext, torchvision and captum (for model interpretability)\n",
    "RUN pip install torchvision torchtext matplotlib tensorboard captum\n",
    "\n",
    "\n",
    "# Apart from PyTorch, other core DL libraries for NLP\n",
    "# installing spacy, transformers and sentence_transformers\n",
    "RUN pip install transformers tokenizers spacy[cuda111,transformers] sentencepiece sentence-transformers\n",
    "\n",
    "# install spacy small and trf models\n",
    "RUN python -m spacy download en_core_web_sm\n",
    "RUN python -m spacy download en_core_web_trf\n",
    "\n",
    "# python ecosystem for traditional machine learning/ data science\n",
    "RUN pip install -U scikit-learn pandas numpy scipy seaborn \n",
    "\n",
    "\n",
    "RUN pip install jupyter\n",
    "\n",
    "# to run jupyter in HPC\n",
    "RUN jupyter nbextension enable --py widgetsnbextension && \\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ca12191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "================= Installed pipeline packages (spaCy v3.3.0) =================\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation: /opt/conda/lib/python3.7/site-packages/spacy\u001b[0m\n",
      "\n",
      "NAME              SPACY                 VERSION                            \n",
      "en_core_web_sm    >=3.3.0.dev0,<3.4.0   \u001b[38;5;2m3.3.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "en_core_web_trf   >=3.3.0.dev0,<3.4.0   \u001b[38;5;2m3.3.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf3df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd00453",
   "metadata": {},
   "source": [
    "### Dir Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "295f02f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/path/to/dir/spacy_model_training_ner/data/diease_ner/train_dev_test_split_conll_data/\"\n",
    "SPACY_DATA_DIR = \"/path/to/dir/spacy_model_training_ner/data/diease_ner/train_dev_test_split_spacy_binary/\"\n",
    "CONFIG_DIR = \"/path/to/dir/spacy_model_training_ner/data/model_config/spacy_roberta_base_model/\" \n",
    "SPACY_ROBERTA_MODEL_DIR_GPU = '/path/to/dir/spacy_model_training_ner/data/model_weights/spacy_roberta_base_/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97054b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_data.conll\ttest_data.conll  train_data.conll\n",
      "dev_data.spacy\ttest_data.spacy  train_data.spacy\n"
     ]
    }
   ],
   "source": [
    "!ls $DATA_DIR\n",
    "!ls $SPACY_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3997137",
   "metadata": {},
   "source": [
    "### Spacy Convert\n",
    "Already done while running spacy small model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b290942",
   "metadata": {},
   "source": [
    "### 2. Spacy Init Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0d7bc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: ner\n",
      "- Optimize for: accuracy\n",
      "- Hardware: GPU\n",
      "- Transformer: roberta-base\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "/path/to/dir/spacy_model_training_ner/data/model_config/spacy_roberta_base_model/original_trf_config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train original_trf_config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config $CONFIG_DIR/original_trf_config.cfg --lang en -G --pipeline \"ner\" --optimize accuracy --force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a16f421",
   "metadata": {},
   "source": [
    "### 3. Debug Spacy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27c68eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================ Data file validation ============================\u001b[0m\n",
      "Downloading: 100%|██████████████████████████████| 481/481 [00:00<00:00, 665kB/s]\n",
      "Downloading: 100%|███████████████████████████| 878k/878k [00:00<00:00, 2.40MB/s]\n",
      "Downloading: 100%|███████████████████████████| 446k/446k [00:00<00:00, 1.47MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.29M/1.29M [00:00<00:00, 3.09MB/s]\n",
      "Downloading: 100%|███████████████████████████| 478M/478M [00:34<00:00, 14.6MB/s]\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[38;5;2m✔ Pipeline can be initialized with data\u001b[0m\n",
      "\u001b[38;5;2m✔ Corpus is loadable\u001b[0m\n",
      "\u001b[1m\n",
      "=============================== Training stats ===============================\u001b[0m\n",
      "Language: en\n",
      "Training pipeline: transformer, ner\n",
      "5836 training docs\n",
      "730 evaluation docs\n",
      "\u001b[38;5;3m⚠ 7 training examples also in evaluation data\u001b[0m\n",
      "\u001b[1m\n",
      "============================== Vocab & Vectors ==============================\u001b[0m\n",
      "\u001b[38;5;4mℹ 148187 total word(s) in the data (9829 unique)\u001b[0m\n",
      "10 most common words: '.' (6708), 'the' (6121), 'of' (5842), ',' (5117), 'in'\n",
      "(4022), '-' (3911), 'and' (3471), 'a' (2532), ')' (2093), '(' (2084)\n",
      "\u001b[38;5;4mℹ No word vectors present in the package\u001b[0m\n",
      "\u001b[1m\n",
      "========================== Named Entity Recognition ==========================\u001b[0m\n",
      "\u001b[38;5;4mℹ 2 label(s)\u001b[0m\n",
      "0 missing value(s) (tokens with '-' label)\n",
      "Labels in train data: 'EP]', 'DISEASE'\n",
      "\u001b[38;5;3m⚠ Low number of examples for label 'EP]' (7)\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[2K\u001b[38;5;2m✔ Examples without occurrences available for all labels\u001b[0m\n",
      "\u001b[38;5;2m✔ No entities consisting of or starting/ending with whitespace\u001b[0m\n",
      "\u001b[38;5;2m✔ No entities crossing sentence boundaries\u001b[0m\n",
      "To train a new entity type, your data should include at least 50 instances of\n",
      "the new label\n",
      "\u001b[1m\n",
      "================================== Summary ==================================\u001b[0m\n",
      "\u001b[38;5;2m✔ 5 checks passed\u001b[0m\n",
      "\u001b[38;5;3m⚠ 2 warnings\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy debug data $CONFIG_DIR/original_trf_config.cfg \\\n",
    "--paths.train $SPACY_DATA_DIR/train_data.spacy \\\n",
    "--paths.dev $SPACY_DATA_DIR/dev_data.spacy \\\n",
    "--paths.dev $SPACY_DATA_DIR/test_data.spacy \\\n",
    "--verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3254c7",
   "metadata": {},
   "source": [
    "###  4. Train Roberta base NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d8b1b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-24 07:44:08,351] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "\u001b[38;5;2m✔ Created output directory:\n",
      "/path/to/dir/SpacyNER/data/model_weights/spacy_roberta_base_\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory:\n",
      "/path/to/dir/SpacyNER/data/model_weights/spacy_roberta_base_\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2022-05-24 07:44:11,169] [INFO] Set up nlp object from config\n",
      "[2022-05-24 07:44:11,178] [DEBUG] Loading corpus from path: /path/to/dir/spacy_model_training_ner/data/diease_ner/train_dev_test_split_spacy_binary/dev_data.spacy\n",
      "[2022-05-24 07:44:11,180] [DEBUG] Loading corpus from path: /path/to/dir/spacy_model_training_ner/data/diease_ner/train_dev_test_split_spacy_binary/train_data.spacy\n",
      "[2022-05-24 07:44:11,180] [INFO] Pipeline: ['transformer', 'ner']\n",
      "[2022-05-24 07:44:11,184] [INFO] Created vocabulary\n",
      "[2022-05-24 07:44:11,185] [INFO] Finished initializing nlp object\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[2022-05-24 07:44:18,928] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, id, lb, mk, pt, ru, sr, ta, th\n",
      "\n",
      "Load the table in your config with:\n",
      "\n",
      "[initialize.lookups]\n",
      "@misc = \"spacy.LookupsDataLoader.v1\"\n",
      "lang = ${nlp.lang}\n",
      "tables = [\"lexeme_norm\"]\n",
      "\n",
      "[2022-05-24 07:44:22,286] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2022-05-24 07:44:22,298] [DEBUG] Loading corpus from path: /path/to/dir/spacy_model_training_ner/data/diease_ner/train_dev_test_split_spacy_binary/dev_data.spacy\n",
      "[2022-05-24 07:44:22,299] [DEBUG] Loading corpus from path: /path/to/dir/spacy_model_training_ner/data/diease_ner/train_dev_test_split_spacy_binary/train_data.spacy\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0        4392.55    285.04    0.21    0.17    0.29    0.00\n",
      "  1     200      126102.97  33471.51   84.94   81.78   88.36    0.85\n",
      "  3     400        1782.34   2642.55   89.50   90.83   88.22    0.90\n",
      "  5     600        1123.23   1596.50   90.69   89.06   92.39    0.91\n",
      "  7     800         704.54    991.96   90.48   90.16   90.80    0.90\n",
      "  9    1000         511.18    708.38   88.91   90.36   87.50    0.89\n",
      " 11    1200         368.91    535.26   89.55   88.61   90.52    0.90\n",
      " 13    1400         311.13    413.71   91.36   90.08   92.67    0.91\n",
      " 15    1600         274.22    377.04   90.87   90.94   90.80    0.91\n",
      " 17    1800         251.61    326.45   91.64   91.18   92.10    0.92\n",
      " 19    2000         209.20    272.41   90.82   89.99   91.67    0.91\n",
      " 21    2200         150.59    200.59   90.17   88.19   92.24    0.90\n",
      " 23    2400         141.67    207.85   91.76   90.73   92.82    0.92\n",
      " 25    2600         113.48    149.51   91.36   90.78   91.95    0.91\n",
      " 27    2800         153.06    176.41   90.94   90.35   91.52    0.91\n",
      " 29    3000         103.80    128.69   90.86   88.98   92.82    0.91\n",
      " 30    3200         121.04    141.70   91.47   90.56   92.39    0.91\n",
      " 32    3400          90.05    116.95   90.51   89.93   91.09    0.91\n",
      " 34    3600         111.25    131.25   91.12   90.15   92.10    0.91\n",
      " 36    3800          79.87     82.69   90.30   89.66   90.95    0.90\n",
      " 38    4000          82.07     82.97   90.97   90.01   91.95    0.91\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "/path/to/dir/SpacyNER/data/model_weights/spacy_roberta_base_/model-last\n",
      "CPU times: user 11.1 s, sys: 2.78 s, total: 13.9 s\n",
      "Wall time: 22min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python3 -m spacy train $CONFIG_DIR/original_trf_config.cfg \\\n",
    "--output $SPACY_ROBERTA_MODEL_DIR_GPU \\\n",
    "--paths.train $SPACY_DATA_DIR/train_data.spacy \\\n",
    "--paths.dev $SPACY_DATA_DIR/dev_data.spacy \\\n",
    "--verbose \\\n",
    "-g 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
