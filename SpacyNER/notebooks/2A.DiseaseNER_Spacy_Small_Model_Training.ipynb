{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7f86055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "================= Installed pipeline packages (spaCy v3.3.0) =================\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation: /opt/conda/lib/python3.7/site-packages/spacy\u001b[0m\n",
      "\n",
      "NAME              SPACY                 VERSION                            \n",
      "en_core_web_sm    >=3.3.0.dev0,<3.4.0   \u001b[38;5;2m3.3.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "en_core_web_trf   >=3.3.0.dev0,<3.4.0   \u001b[38;5;2m3.3.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e25e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cae2df",
   "metadata": {},
   "source": [
    "### Dir Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5704a817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_data.conll\ttest_data.conll  train_data.conll\r\n"
     ]
    }
   ],
   "source": [
    "!ls /path/to/dir/spacy_model_training_ner/data/diease_ner/train_dev_test_split_conll_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2b678de",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/path/to/dir/spacy_model_training_ner/data/diease_ner/train_dev_test_split_conll_data/\"\n",
    "SPACY_DATA_DIR = \"/path/to/dir/spacy_model_training_ner/data/diease_ner/train_dev_test_split_spacy_binary/\"\n",
    "CONFIG_DIR = \"/path/to/dir/spacy_model_training_ner/data/model_config/spacy_small_model/\" \n",
    "SPACY_SMALL_MODEL_DIR_GPU = '/path/to/dir/spacy_model_training_ner/data/model_weights/spacy_small/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0f2d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $SPACY_DATA_DIR $CONFIG_DIR $SPACY_SMALL_MODEL_DIR_GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967d3a1c",
   "metadata": {},
   "source": [
    "### 1. Spacy Convert\n",
    "- conll to spacy compatible format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14f7aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Auto-detected token-per-line NER format\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n",
      "into documents with `-n 10`.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (5836 documents):\n",
      "/path/to/dir/spacy_model_training_ner/data/diease_ner/train_dev_test_split_spacy_binary/train_data.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Auto-detected token-per-line NER format\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n",
      "into documents with `-n 10`.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (729 documents):\n",
      "/path/to/dir/spacy_model_training_ner/data/diease_ner/train_dev_test_split_spacy_binary/dev_data.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Auto-detected token-per-line NER format\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n",
      "into documents with `-n 10`.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (730 documents):\n",
      "/path/to/dir/spacy_model_training_ner/data/diease_ner/train_dev_test_split_spacy_binary/test_data.spacy\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy convert $DATA_DIR/train_data.conll $SPACY_DATA_DIR -c ner -n 1\n",
    "!python3 -m spacy convert $DATA_DIR/dev_data.conll $SPACY_DATA_DIR -c ner -n 1\n",
    "!python3 -m spacy convert $DATA_DIR/test_data.conll $SPACY_DATA_DIR -c iob -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c558316b",
   "metadata": {},
   "source": [
    "### 2. Spacy Init Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6049283d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: ner\n",
      "- Optimize for: efficiency\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "/path/to/dir/spacy_model_training_ner/data/model_config/spacy_small_model/original_spacy_small_ner_config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train original_spacy_small_ner_config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy init config $CONFIG_DIR/original_spacy_small_ner_config.cfg --lang en --pipeline \"ner\" --optimize efficiency --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a355272a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[paths]\r\n",
      "train = null\r\n",
      "dev = null\r\n",
      "vectors = null\r\n",
      "init_tok2vec = null\r\n",
      "\r\n",
      "[system]\r\n",
      "gpu_allocator = null\r\n",
      "seed = 0\r\n",
      "\r\n",
      "[nlp]\r\n",
      "lang = \"en\"\r\n",
      "pipeline = [\"tok2vec\",\"ner\"]\r\n",
      "batch_size = 1000\r\n",
      "disabled = []\r\n",
      "before_creation = null\r\n",
      "after_creation = null\r\n",
      "after_pipeline_creation = null\r\n",
      "tokenizer = {\"@tokenizers\":\"spacy.Tokenizer.v1\"}\r\n",
      "\r\n",
      "[components]\r\n",
      "\r\n",
      "[components.ner]\r\n",
      "factory = \"ner\"\r\n",
      "incorrect_spans_key = null\r\n",
      "moves = null\r\n",
      "scorer = {\"@scorers\":\"spacy.ner_scorer.v1\"}\r\n",
      "update_with_oracle_cut_size = 100\r\n",
      "\r\n",
      "[components.ner.model]\r\n",
      "@architectures = \"spacy.TransitionBasedParser.v2\"\r\n",
      "state_type = \"ner\"\r\n",
      "extra_state_tokens = false\r\n",
      "hidden_width = 64\r\n",
      "maxout_pieces = 2\r\n",
      "use_upper = true\r\n",
      "nO = null\r\n",
      "\r\n",
      "[components.ner.model.tok2vec]\r\n",
      "@architectures = \"spacy.Tok2VecListener.v1\"\r\n",
      "width = ${components.tok2vec.model.encode.width}\r\n",
      "upstream = \"*\"\r\n",
      "\r\n",
      "[components.tok2vec]\r\n",
      "factory = \"tok2vec\"\r\n",
      "\r\n",
      "[components.tok2vec.model]\r\n",
      "@architectures = \"spacy.Tok2Vec.v2\"\r\n",
      "\r\n",
      "[components.tok2vec.model.embed]\r\n",
      "@architectures = \"spacy.MultiHashEmbed.v2\"\r\n",
      "width = ${components.tok2vec.model.encode.width}\r\n",
      "attrs = [\"NORM\",\"PREFIX\",\"SUFFIX\",\"SHAPE\"]\r\n",
      "rows = [5000,2500,2500,2500]\r\n",
      "include_static_vectors = false\r\n",
      "\r\n",
      "[components.tok2vec.model.encode]\r\n",
      "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\r\n",
      "width = 96\r\n",
      "depth = 4\r\n",
      "window_size = 1\r\n",
      "maxout_pieces = 3\r\n",
      "\r\n",
      "[corpora]\r\n",
      "\r\n",
      "[corpora.dev]\r\n",
      "@readers = \"spacy.Corpus.v1\"\r\n",
      "path = ${paths.dev}\r\n",
      "max_length = 0\r\n",
      "gold_preproc = false\r\n",
      "limit = 0\r\n",
      "augmenter = null\r\n",
      "\r\n",
      "[corpora.train]\r\n",
      "@readers = \"spacy.Corpus.v1\"\r\n",
      "path = ${paths.train}\r\n",
      "max_length = 0\r\n",
      "gold_preproc = false\r\n",
      "limit = 0\r\n",
      "augmenter = null\r\n",
      "\r\n",
      "[training]\r\n",
      "dev_corpus = \"corpora.dev\"\r\n",
      "train_corpus = \"corpora.train\"\r\n",
      "seed = ${system.seed}\r\n",
      "gpu_allocator = ${system.gpu_allocator}\r\n",
      "dropout = 0.1\r\n",
      "accumulate_gradient = 1\r\n",
      "patience = 1600\r\n",
      "max_epochs = 0\r\n",
      "max_steps = 20000\r\n",
      "eval_frequency = 200\r\n",
      "frozen_components = []\r\n",
      "annotating_components = []\r\n",
      "before_to_disk = null\r\n",
      "\r\n",
      "[training.batcher]\r\n",
      "@batchers = \"spacy.batch_by_words.v1\"\r\n",
      "discard_oversize = false\r\n",
      "tolerance = 0.2\r\n",
      "get_length = null\r\n",
      "\r\n",
      "[training.batcher.size]\r\n",
      "@schedules = \"compounding.v1\"\r\n",
      "start = 100\r\n",
      "stop = 1000\r\n",
      "compound = 1.001\r\n",
      "t = 0.0\r\n",
      "\r\n",
      "[training.logger]\r\n",
      "@loggers = \"spacy.ConsoleLogger.v1\"\r\n",
      "progress_bar = false\r\n",
      "\r\n",
      "[training.optimizer]\r\n",
      "@optimizers = \"Adam.v1\"\r\n",
      "beta1 = 0.9\r\n",
      "beta2 = 0.999\r\n",
      "L2_is_weight_decay = true\r\n",
      "L2 = 0.01\r\n",
      "grad_clip = 1.0\r\n",
      "use_averages = false\r\n",
      "eps = 0.00000001\r\n",
      "learn_rate = 0.001\r\n",
      "\r\n",
      "[training.score_weights]\r\n",
      "ents_f = 1.0\r\n",
      "ents_p = 0.0\r\n",
      "ents_r = 0.0\r\n",
      "ents_per_type = null\r\n",
      "\r\n",
      "[pretraining]\r\n",
      "\r\n",
      "[initialize]\r\n",
      "vectors = ${paths.vectors}\r\n",
      "init_tok2vec = ${paths.init_tok2vec}\r\n",
      "vocab_data = null\r\n",
      "lookups = null\r\n",
      "before_init = null\r\n",
      "after_init = null\r\n",
      "\r\n",
      "[initialize.components]\r\n",
      "\r\n",
      "[initialize.tokenizer]"
     ]
    }
   ],
   "source": [
    "!cat $CONFIG_DIR/original_spacy_small_ner_config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbed05b",
   "metadata": {},
   "source": [
    "**Changes made to the config**: <br> \n",
    "- the dropout from 0.1 to 0.3 to make memorizing of the Rules NER output difficult\n",
    "<!-- - Added pipeline components like tagger, parser (needed for subsequent Rules NER) \n",
    "    - Also add them to frozen_components = [\"tagger\",\"parser\"] -->\n",
    "- Some important training pieces\n",
    "```\n",
    "patience = 3000\n",
    "max_epochs = 50\n",
    "max_steps = 20000\n",
    "```\n",
    "- Changed Console logger `progress_bar` to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b842ad6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /path/to/dir/spacy_model_training_ner/data/model_config/spacy_small_model//original_spacy_small_ner_config.cfg\n"
     ]
    }
   ],
   "source": [
    "%%writefile $CONFIG_DIR/original_spacy_small_ner_config.cfg\n",
    "[paths]\n",
    "train = null\n",
    "dev = null\n",
    "vectors = null\n",
    "init_tok2vec = null\n",
    "\n",
    "[system]\n",
    "gpu_allocator = null\n",
    "seed = 0\n",
    "\n",
    "[nlp]\n",
    "lang = \"en\"\n",
    "pipeline = [\"tok2vec\",\"ner\"]\n",
    "batch_size = 1000\n",
    "disabled = []\n",
    "before_creation = null\n",
    "after_creation = null\n",
    "after_pipeline_creation = null\n",
    "tokenizer = {\"@tokenizers\":\"spacy.Tokenizer.v1\"}\n",
    "\n",
    "[components]\n",
    "\n",
    "[components.ner]\n",
    "factory = \"ner\"\n",
    "incorrect_spans_key = null\n",
    "moves = null\n",
    "scorer = {\"@scorers\":\"spacy.ner_scorer.v1\"}\n",
    "update_with_oracle_cut_size = 100\n",
    "\n",
    "[components.ner.model]\n",
    "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
    "state_type = \"ner\"\n",
    "extra_state_tokens = false\n",
    "hidden_width = 64\n",
    "maxout_pieces = 2\n",
    "use_upper = true\n",
    "nO = null\n",
    "\n",
    "[components.ner.model.tok2vec]\n",
    "@architectures = \"spacy.Tok2VecListener.v1\"\n",
    "width = ${components.tok2vec.model.encode.width}\n",
    "upstream = \"*\"\n",
    "\n",
    "[components.tok2vec]\n",
    "factory = \"tok2vec\"\n",
    "\n",
    "[components.tok2vec.model]\n",
    "@architectures = \"spacy.Tok2Vec.v2\"\n",
    "\n",
    "[components.tok2vec.model.embed]\n",
    "@architectures = \"spacy.MultiHashEmbed.v2\"\n",
    "width = ${components.tok2vec.model.encode.width}\n",
    "attrs = [\"NORM\",\"PREFIX\",\"SUFFIX\",\"SHAPE\"]\n",
    "rows = [5000,2500,2500,2500]\n",
    "include_static_vectors = false\n",
    "\n",
    "[components.tok2vec.model.encode]\n",
    "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
    "width = 96\n",
    "depth = 4\n",
    "window_size = 1\n",
    "maxout_pieces = 3\n",
    "\n",
    "[corpora]\n",
    "\n",
    "[corpora.dev]\n",
    "@readers = \"spacy.Corpus.v1\"\n",
    "path = ${paths.dev}\n",
    "max_length = 0\n",
    "gold_preproc = false\n",
    "limit = 0\n",
    "augmenter = null\n",
    "\n",
    "[corpora.train]\n",
    "@readers = \"spacy.Corpus.v1\"\n",
    "path = ${paths.train}\n",
    "max_length = 0\n",
    "gold_preproc = false\n",
    "limit = 0\n",
    "augmenter = null\n",
    "\n",
    "[training]\n",
    "dev_corpus = \"corpora.dev\"\n",
    "train_corpus = \"corpora.train\"\n",
    "seed = ${system.seed}\n",
    "gpu_allocator = ${system.gpu_allocator}\n",
    "dropout = 0.3\n",
    "accumulate_gradient = 1\n",
    "patience = 3000\n",
    "max_epochs = 50\n",
    "max_steps = 20000\n",
    "eval_frequency = 200\n",
    "frozen_components = []\n",
    "annotating_components = []\n",
    "before_to_disk = null\n",
    "\n",
    "[training.batcher]\n",
    "@batchers = \"spacy.batch_by_words.v1\"\n",
    "discard_oversize = false\n",
    "tolerance = 0.2\n",
    "get_length = null\n",
    "\n",
    "[training.batcher.size]\n",
    "@schedules = \"compounding.v1\"\n",
    "start = 100\n",
    "stop = 1000\n",
    "compound = 1.001\n",
    "t = 0.0\n",
    "\n",
    "[training.logger]\n",
    "@loggers = \"spacy.ConsoleLogger.v1\"\n",
    "progress_bar = true\n",
    "\n",
    "[training.optimizer]\n",
    "@optimizers = \"Adam.v1\"\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "L2_is_weight_decay = true\n",
    "L2 = 0.01\n",
    "grad_clip = 1.0\n",
    "use_averages = false\n",
    "eps = 0.00000001\n",
    "learn_rate = 0.001\n",
    "\n",
    "[training.score_weights]\n",
    "ents_f = 1.0\n",
    "ents_p = 0.0\n",
    "ents_r = 0.0\n",
    "ents_per_type = null\n",
    "\n",
    "[pretraining]\n",
    "\n",
    "[initialize]\n",
    "vectors = ${paths.vectors}\n",
    "init_tok2vec = ${paths.init_tok2vec}\n",
    "vocab_data = null\n",
    "lookups = null\n",
    "before_init = null\n",
    "after_init = null\n",
    "\n",
    "[initialize.components]\n",
    "\n",
    "[initialize.tokenizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "029cbeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paths', 'system', 'nlp', 'components', 'components.ner', 'components.ner.model', 'components.ner.model.tok2vec', 'components.tok2vec', 'components.tok2vec.model', 'components.tok2vec.model.embed', 'components.tok2vec.model.encode', 'corpora', 'corpora.dev', 'corpora.train', 'training', 'training.batcher', 'training.batcher.size', 'training.logger', 'training.optimizer', 'training.score_weights', 'pretraining', 'initialize', 'initialize.components', 'initialize.tokenizer']\n"
     ]
    }
   ],
   "source": [
    "from configparser import ConfigParser\n",
    "configur = ConfigParser()\n",
    "configur.read(f'{CONFIG_DIR}/original_spacy_small_ner_config.cfg')\n",
    "print(configur.sections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87cef465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@architectures': '\"spacy.TransitionBasedParser.v2\"',\n",
       " 'state_type': '\"ner\"',\n",
       " 'extra_state_tokens': 'false',\n",
       " 'hidden_width': '64',\n",
       " 'maxout_pieces': '2',\n",
       " 'use_upper': 'true',\n",
       " 'no': 'null'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type of NER model\n",
    "dict(configur.items(\"components.ner.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fd3dfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nlp]\r\n",
      "lang = \"en\"\r\n",
      "pipeline = [\"tok2vec\",\"ner\"]\r\n",
      "batch_size = 1000\r\n",
      "disabled = []\r\n",
      "before_creation = null\r\n",
      "after_creation = null\r\n",
      "after_pipeline_creation = null\r\n",
      "tokenizer = {\"@tokenizers\":\"spacy.Tokenizer.v1\"}\r\n",
      "\r\n",
      "[components]\r\n",
      "\r\n",
      "[components.ner]\r\n",
      "factory = \"ner\"\r\n",
      "incorrect_spans_key = null\r\n",
      "moves = null\r\n",
      "scorer = {\"@scorers\":\"spacy.ner_scorer.v1\"}\r\n",
      "update_with_oracle_cut_size = 100\r\n",
      "\r\n",
      "[components.ner.model]\r\n",
      "@architectures = \"spacy.TransitionBasedParser.v2\"\r\n",
      "state_type = \"ner\"\r\n",
      "extra_state_tokens = false\r\n",
      "hidden_width = 64\r\n",
      "maxout_pieces = 2\r\n",
      "use_upper = true\r\n",
      "nO = null\r\n",
      "\r\n",
      "[components.ner.model.tok2vec]\r\n",
      "@architectures = \"spacy.Tok2VecListener.v1\"\r\n",
      "width = ${components.tok2vec.model.encode.width}\r\n",
      "upstream = \"*\"\r\n",
      "\r\n",
      "[components.tok2vec]\r\n",
      "factory = \"tok2vec\"\r\n",
      "\r\n",
      "[components.tok2vec.model]\r\n",
      "@architectures = \"spacy.Tok2Vec.v2\"\r\n",
      "\r\n",
      "[components.tok2vec.model.embed]\r\n",
      "@architectures = \"spacy.MultiHashEmbed.v2\"\r\n",
      "width = ${components.tok2vec.model.encode.width}\r\n",
      "attrs = [\"NORM\",\"PREFIX\",\"SUFFIX\",\"SHAPE\"]\r\n",
      "rows = [5000,2500,2500,2500]\r\n",
      "include_static_vectors = false\r\n",
      "\r\n",
      "[components.tok2vec.model.encode]\r\n",
      "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\r\n",
      "width = 96\r\n",
      "depth = 4\r\n"
     ]
    }
   ],
   "source": [
    "# critical config changes\n",
    "!head -n 60 $CONFIG_DIR/original_spacy_small_ner_config.cfg | tail -n 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3948811c",
   "metadata": {},
   "source": [
    "### 4. Debug Spacy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45181229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================ Data file validation ============================\u001b[0m\n",
      "\u001b[38;5;2m✔ Pipeline can be initialized with data\u001b[0m\n",
      "\u001b[38;5;2m✔ Corpus is loadable\u001b[0m\n",
      "\u001b[1m\n",
      "=============================== Training stats ===============================\u001b[0m\n",
      "Language: en\n",
      "Training pipeline: tok2vec, ner\n",
      "5836 training docs\n",
      "729 evaluation docs\n",
      "\u001b[38;5;3m⚠ 5 training examples also in evaluation data\u001b[0m\n",
      "\u001b[1m\n",
      "============================== Vocab & Vectors ==============================\u001b[0m\n",
      "\u001b[38;5;4mℹ 148187 total word(s) in the data (9829 unique)\u001b[0m\n",
      "10 most common words: '.' (6708), 'the' (6121), 'of' (5842), ',' (5117), 'in'\n",
      "(4022), '-' (3911), 'and' (3471), 'a' (2532), ')' (2093), '(' (2084)\n",
      "\u001b[38;5;4mℹ No word vectors present in the package\u001b[0m\n",
      "\u001b[1m\n",
      "========================== Named Entity Recognition ==========================\u001b[0m\n",
      "\u001b[38;5;4mℹ 2 label(s)\u001b[0m\n",
      "0 missing value(s) (tokens with '-' label)\n",
      "Labels in train data: 'EP]', 'DISEASE'\n",
      "\u001b[38;5;3m⚠ Low number of examples for label 'EP]' (7)\u001b[0m\n",
      "\u001b[2K\u001b[38;5;2m✔ Examples without occurrences available for all labels\u001b[0m\n",
      "\u001b[38;5;2m✔ No entities consisting of or starting/ending with whitespace\u001b[0m\n",
      "\u001b[38;5;2m✔ No entities crossing sentence boundaries\u001b[0m\n",
      "To train a new entity type, your data should include at least 50 instances of\n",
      "the new label\n",
      "\u001b[1m\n",
      "================================== Summary ==================================\u001b[0m\n",
      "\u001b[38;5;2m✔ 5 checks passed\u001b[0m\n",
      "\u001b[38;5;3m⚠ 2 warnings\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy debug data $CONFIG_DIR/original_spacy_small_ner_config.cfg \\\n",
    "--paths.train $SPACY_DATA_DIR/train_data.spacy \\\n",
    "--paths.dev $SPACY_DATA_DIR/dev_data.spacy \\\n",
    "--paths.test $SPACY_DATA_DIR/test_data.spacy \\\n",
    "--verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bb532b",
   "metadata": {},
   "source": [
    "### 5. Train Custom NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd744c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-24 07:09:39,410] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "\u001b[38;5;4mℹ Saving to output directory:\n",
      "/path/to/dir/SpacyNER/data/model_weights/spacy_small\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2022-05-24 07:09:41,789] [INFO] Set up nlp object from config\n",
      "[2022-05-24 07:09:41,797] [DEBUG] Loading corpus from path: /path/to/dir/spacy_model_training_ner/data/diease_ner/train_dev_test_split_spacy_binary/dev_data.spacy\n",
      "[2022-05-24 07:09:41,798] [DEBUG] Loading corpus from path: /path/to/dir/spacy_model_training_ner/data/diease_ner/train_dev_test_split_spacy_binary/train_data.spacy\n",
      "[2022-05-24 07:09:41,798] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2022-05-24 07:09:41,803] [INFO] Created vocabulary\n",
      "[2022-05-24 07:09:41,804] [INFO] Finished initializing nlp object\n",
      "[2022-05-24 07:09:46,631] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, id, lb, mk, pt, ru, sr, ta, th\n",
      "\n",
      "Load the table in your config with:\n",
      "\n",
      "[initialize.lookups]\n",
      "@misc = \"spacy.LookupsDataLoader.v1\"\n",
      "lang = ${nlp.lang}\n",
      "tables = [\"lexeme_norm\"]\n",
      "\n",
      "[2022-05-24 07:09:51,839] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2022-05-24 07:09:51,852] [DEBUG] Loading corpus from path: /path/to/dir/spacy_model_training_ner/data/diease_ner/train_dev_test_split_spacy_binary/dev_data.spacy\n",
      "[2022-05-24 07:09:51,853] [DEBUG] Loading corpus from path: /path/to/dir/spacy_model_training_ner/data/diease_ner/train_dev_test_split_spacy_binary/train_data.spacy\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     41.00    0.31    0.33    0.29    0.00\n",
      "  0     200        195.83   1820.96   41.44   55.56   33.05    0.41             \n",
      "  0     400        106.20   1131.87   47.27   66.58   36.64    0.47             \n",
      "  0     600         64.99    969.69   74.94   77.17   72.84    0.75             \n",
      "  0     800         87.66   1096.80   74.39   76.96   71.98    0.74             \n",
      "  1    1000         82.51   1134.93   77.53   79.28   75.86    0.78             \n",
      "  1    1200        113.61   1122.73   80.73   82.36   79.17    0.81             \n",
      "  1    1400        128.84   1178.08   84.40   86.10   82.76    0.84             \n",
      "  2    1600        128.56   1168.60   84.36   85.42   83.33    0.84             \n",
      "  3    1800        198.99   1418.31   86.08   85.96   86.21    0.86             \n",
      "  4    2000        229.61   1330.86   85.73   85.13   86.35    0.86             \n",
      "  5    2200        219.49   1438.67   84.99   85.36   84.63    0.85             \n",
      "  6    2400        287.36   1494.78   85.41   85.04   85.78    0.85             \n",
      "  7    2600        285.34   1292.50   85.40   86.34   84.48    0.85             \n",
      "  9    2800        358.33   1274.73   87.62   89.39   85.92    0.88             \n",
      " 10    3000        398.65   1110.96   86.56   87.52   85.63    0.87             \n",
      " 11    3200        296.68   1070.00   86.96   86.71   87.21    0.87             \n",
      " 13    3400        288.34    913.51   87.43   88.99   85.92    0.87             \n",
      " 14    3600        247.69    891.84   86.96   87.72   86.21    0.87             \n",
      " 15    3800        594.71    909.90   86.64   86.64   86.64    0.87             \n",
      " 17    4000        263.25    883.97   86.83   87.01   86.64    0.87             \n",
      " 18    4200        323.90    745.82   86.46   87.15   85.78    0.86             \n",
      " 19    4400        272.63    759.72   85.92   85.92   85.92    0.86             \n",
      " 21    4600        293.18    665.94   85.94   86.25   85.63    0.86             \n",
      " 22    4800        509.87    650.19   87.10   87.87   86.35    0.87             \n",
      " 23    5000        283.07    644.34   85.90   86.46   85.34    0.86             \n",
      " 25    5200        304.16    687.46   84.56   84.93   84.20    0.85             \n",
      " 26    5400        287.07    591.02   85.96   87.04   84.91    0.86             \n",
      " 27    5600        382.16    540.78   86.21   87.56   84.91    0.86             \n",
      " 28    5800        406.03    615.57   86.29   86.67   85.92    0.86             \n",
      "Epoch 29:   0%|                                         | 0/200 [00:00<?, ?it/s]\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "/path/to/dir/SpacyNER/data/model_weights/spacy_small/model-last\n",
      "CPU times: user 4.81 s, sys: 1.21 s, total: 6.02 s                              \n",
      "Wall time: 7min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python3 -m spacy train $CONFIG_DIR/original_spacy_small_ner_config.cfg \\\n",
    "--output $SPACY_SMALL_MODEL_DIR_GPU \\\n",
    "--paths.train $SPACY_DATA_DIR/train_data.spacy \\\n",
    "--paths.dev $SPACY_DATA_DIR/dev_data.spacy \\\n",
    "--verbose \\\n",
    "-g 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
